# -*- coding: utf-8 -*-
"""ACDS-Project.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1JDQAPgi_il_yZY902DHYf_x-osOUWyMt
"""

# Importing different packages
import requests
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt

############################ FUNCTIONS #############################
from numpy.random import seed
from numpy.random import randn
from numpy.random import rand
from numpy import append
import matplotlib.pyplot as plt
from numpy import exp
import pandas as pd
from scipy.stats import boxcox
import numpy as np
from numpy import array
from scipy.linalg import svd
from numpy import array
from numpy import diag
from numpy import dot
from numpy import zeros
import numpy
from pandas.plotting import scatter_matrix
import matplotlib.pyplot as plt
import seaborn as sns
from numpy import array
from sklearn.decomposition import PCA
from numpy import set_printoptions
from sklearn.feature_selection import SelectKBest
from sklearn.feature_selection import chi2, f_classif, mutual_info_classif, f_regression, mutual_info_regression
from pandas import read_csv
from sklearn.feature_selection import RFE
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import ExtraTreesClassifier
from sklearn.decomposition import FastICA
from sklearn.neighbors import NeighborhoodComponentsAnalysis


##### Removing or Replacing missing values
def missing_values(dataframe, mv_list=[99999]):
	df_withNaN = dataframe
	df_withNaN.iloc[:, range(len(dataframe.columns) - 1)] = df_withNaN.iloc[:, range(len(dataframe.columns) - 1)].replace(mv_list, numpy.NaN)
	# Finding the rows with missing values
	mv_indices = df_withNaN.loc[(df_withNaN.isnull()).any(axis=1), :].index.tolist()
	# Counting missing data in each column
	count_mv = df_withNaN.isnull().sum()
	# Removing missing data
	df_NoNaN = df_withNaN.dropna()
	# Replacing with mean
	df_NaN_to_mean = df_withNaN.fillna(df_withNaN.mean())
	return (df_NoNaN, df_NaN_to_mean, [mv_indices, count_mv])

##### Function for Removing Outliers
def outliers(dataframe, equation='3sigma', sel_cols=[]):
	if len(sel_cols) == 0:
		selected_cols_ind = list(range(len(dataframe.columns) -1))
	else:
		selected_cols_ind = [list(dataframe.columns).index(col) for col in sel_cols]
	colns_with_outliers = []
	indicesList = []
	coln_names = []
	sum_outlier = []
	coln_indices = []
	for i in selected_cols_ind: #range(len(dataframe.columns) - 1):
		if (equation == '3sigma'):
			### Using 3 sigma for outliers 
			outlier_coln = numpy.abs((dataframe.iloc[:, [i]] - (dataframe.iloc[:, [i]].mean())) >  (3 * dataframe.iloc[:, [i]].std()))
			indices_coln = outlier_coln.loc[(outlier_coln == True).any(axis=1), :].index.tolist()
		elif (equation == 'IQRE'):
			#### Using max > Q3 + 1.5(Q3 - Q1) and min < Q1 - 1.5(Q3 - Q1) as outliers
			outlier_max = dataframe.iloc[:, [i]] > (dataframe.iloc[:, [i]].quantile(0.75) + 1.5 * (dataframe.iloc[:, [i]].quantile(0.75) - dataframe.iloc[:, [i]].quantile(0.25)))
			outlier_min = dataframe.iloc[:, [i]] < (dataframe.iloc[:, [i]].quantile(0.25) - 1.5 * (dataframe.iloc[:, [i]].quantile(0.75) - dataframe.iloc[:, [i]].quantile(0.25)))
			indices_max = outlier_max.loc[(outlier_max == True).any(axis=1), :].index.tolist()
			indices_min = outlier_min.loc[(outlier_min == True).any(axis=1), :].index.tolist()
			indices_coln = indices_max + indices_min
		else:
			print('=> Error: Invalid argument entry for equation.')
		coln_names.append(dataframe.columns[i])
		sum_outlier.append(len(indices_coln))
		coln_indices.append(indices_coln)
		if len(indices_coln) > 0:
			colns_with_outliers.append(i)
			indicesList = indicesList + indices_coln
	# Creating dataframes for outlier indices for each column
	outlier_sum_coln = pd.DataFrame({'Column' : coln_names, 'Sum' : sum_outlier})
	df_coln_indices = pd.DataFrame({'Column' : coln_names, 'Outlier_Indices' : coln_indices})
	# Removing duplicate indices
	unique_list = []
	for x in indicesList:
		if x not in unique_list:
			unique_list.append(x)
	# Removing outliers for final preprocessed data
	dataframe_Cleaned = dataframe.drop(unique_list)	# Dataset without both outliers and missing data
	return(dataframe_Cleaned, [colns_with_outliers, outlier_sum_coln, df_coln_indices])

##### Clean data
def clean_data(dataframe, mv_list, output='removeNaN', equation='3sigma', selected_cols=[], details= 'no'):
	df_output = None
	df_remove_NaN, df_replace_NaN, mv_details = missing_values(dataframe, mv_list)
	if (output == 'removeNaN'):
		df_output, out_details = outliers(df_remove_NaN, equation, sel_cols=selected_cols)
	elif (output == 'replaceNaN'):
		df_output, out_details = outliers(df_replace_NaN, equation, sel_cols=selected_cols)	
	else:
		df_output = '=> Error: Invalid input for the argument-output'
	if (details== 'no' and (output == 'removeNaN' or output== 'replaceNaN')):
		print('=> Details of missing values and outliers were opted out. \n')
		#print('* Preprocessed Data:\n')
	elif (details== 'yes' and (output == 'removeNaN' or output== 'replaceNaN')):
		print('#################### Missing Values Details ####################\n')
		print('* Rows or indices with missing values:\n\n', mv_details[0])
		print()
		print('* Sum of missing values per column:\n\nColumn\t\tSum \n', mv_details[1])
		print('\n')
		print('#################### Outliers Details ####################\n')
		print('* Columns with outliers:\n\n', out_details[0])
		print()
		print('* Sum of outliers per column:\n\n', out_details[1])
		print()
		print('* Indices of outlier per column:\n\n', out_details[2])
		print('\n')
		#print('* Preprocessed Data:\n')
	else:
		print('=> Error: Invalid input for the argument-details.\n\n')
		#print('* Preprocessed Data:\n')
	return df_output

# Function for ranking element in a list
def calculate_rank(vector):
  a={}
  rank=1
  for num in sorted(vector):
    if num not in a:
      a[num]=rank
      rank=rank+1
  return[a[i] for i in vector]

# Function for finding top largest numbers in a list
def Nmaxelements(list1, N):
	cnt = 0
	final_list = []
	#list_indices = []
	for i in range(0, N):  
		max1 = 0
		for j in range(len(list1)):      
			if list1[j] > max1: 
				max1 = list1[j] 
		final_list.append(max1)
		#list_indices.append(list1.index(max1))
		list1.remove(max1)
		#cnt += 1
	return final_list

def feature_engineering(dataframe, n_features= 3, transform='PCA', score_fn=chi2, details='no'):
	array = dataframe.values
	X = array[:, 0:(len(array[1])-1)]
	Y = array[:,(len(array[1])-1)]

	### PCA Transformation
	if (transform == 'PCA'):
		pca = PCA(n_features)
		# fit on data
		pca.fit(dataframe.iloc[:, 0:(len(dataframe.columns)-1)])
		B = pca.transform(dataframe.iloc[:, 0:(len(dataframe.columns)-1)])
		pca_det = '''
******** PCA details *********\n
* PCA Components:\n
{0}\n		
* PCA Explained variance:
{1}\n					
* PCA Ratios of explained variance:
{2}\n
* Sum of PCA ratios:
{3}\n
'''.format(pca.components_, [round(elem,6) for elem in (pca.explained_variance_)], [round(elem,6) for elem in (pca.explained_variance_ratio_)], round(sum(pca.explained_variance_ratio_),4))
		if (details == 'yes'):
			print(pca_det)
		elif (details == 'no'):
			print('No request for PCA Details.\n')
		else:
			print('=> Error: Invalid input for the argument-details.\n')
		pca_data = pd.DataFrame(B, columns= ['Component_'+ str(x) for x in range(1,n_features+1)])
		pca_data['D_Variable'] = array[:,(len(array[1])-1)]
		return pca_data

	### Univariate Feature Selection
	elif (transform == 'UFS'):
		# score_func= [chi2, f_classif, mutual_info_classif, f_regression, mutual_info_regression]
		test = SelectKBest(score_func=score_fn, k=n_features)
		fit = test.fit(X, Y)
		# summarize scores
		set_printoptions(precision=3)
		#print(fit.scores_)
		features = fit.transform(X)
		f_scores = fit.scores_
		ufs_det= f'Feature scores:\n\n {f_scores}\n'
		rank_scores = calculate_rank(f_scores) # Full scores ranking list
		rank_scores_reduced = calculate_rank(f_scores) # Reduced scores ranking list due to selecting top N numbers
		ufs_n_largest_f = Nmaxelements(rank_scores_reduced, n_features) # Top N features
		rank_index = [rank_scores.index(x) for x in ufs_n_largest_f] # indices of the N selected features
		ufe_sel_colns = dataframe.columns[rank_index].tolist() # Names of feature or coln names
		ufs_rank_det= f'Feature ranking:\n\n {rank_scores}\n'
		if (details == 'yes'):
			print('******** UFS details *********\n')
			print(ufs_det)
			print(ufs_rank_det)
		elif (details == 'no'):
			print('No request for UFS Details.\n')
		else:
			print('=> Error: Invalid input for the argument-details.\n')
		ufs_data = pd.DataFrame(features, columns= [x for x in ufe_sel_colns])
		ufs_data['D_Variable'] = array[:,(len(array[1])-1)]
		return ufs_data

	### Recurssive Feature Elimination
	elif (transform == 'RFE'):
		model = LogisticRegression(solver='lbfgs', max_iter=1000)
		rfe = RFE(model, n_features)
		fit = rfe.fit(X, Y)
		rfe_det1= f'Selected Features:\n\n {fit.support_}\n'
		rfe_det2= f'Feature Ranking:\n\n {fit.ranking_}\n'
		rfe_rank_list = fit.ranking_.tolist()
		sel_att_indices = []
		count = 0
		for x in rfe_rank_list:
			if x == 1:
				sel_att_indices.append(count)
			count += 1	
		rfe_sel_coln = dataframe.columns[sel_att_indices].tolist()
		rfe_sel_coln.append(dataframe.columns[len(dataframe.columns)-1])
		if (details == 'yes'):
			print('******** RFE details *********\n')
			print(rfe_det1)
			print(rfe_det2)
		elif (details == 'no'):
			print('No request for RFE Details.\n')
		else:
			print('=> Error: Invalid input for the argument-details.\n')
		rfe_data = dataframe[rfe_sel_coln]
		return rfe_data

	### Feature Importance Extraction
	elif (transform == 'FIE'):
		model = ExtraTreesClassifier()
		model.fit(X, Y)
		fie_scores = model.feature_importances_
		fie_rank = calculate_rank(fie_scores)
		fie_rank_reduced = calculate_rank(fie_scores)
		fie_det1= f'Feature Importance:\n\n {fie_scores}\n'
		fie_det2= f'Feature Ranking:\n\n {fie_rank}\n'
		fie_n_largest_f = Nmaxelements(fie_rank_reduced, n_features)
		fie_sel_index = [fie_rank.index(x) for x in fie_n_largest_f]
		fie_sel_cols = dataframe.columns[fie_sel_index].tolist()
		fie_sel_cols.append(dataframe.columns[len(dataframe.columns)-1])
		if (details == 'yes'):
			print('******** FIE details *********\n')
			print(fie_det1)
			print(fie_det2)
		elif (details == 'no'):
			print('No request for FIE Details.\n')
		else:
			print('=> Error: Invalid input for the argument-details.\n')
		fie_data = dataframe[fie_sel_cols]
		return fie_data

	### Independent Component Analysis
	elif (transform == 'ICA'):
		ica = FastICA(n_components=n_features, random_state=0)
		ica.fit(X, Y)
		ica_transf_X = ica.transform(X)
		ica_det= '''
******** ICA Details *********\n
* ICA Number of Iterations:\n
{0}\n		
* ICA Components:\n
{1}\n					
* ICA Mixing Matrix:\n
{2}\n
'''.format(ica.n_iter_, ica.components_, ica.mixing_)
		if (details == 'yes'):
			print(ica_det)
		elif (details == 'no'):
			print('No request for ICA Details.\n')
		else:
			print('=> Error: Invalid input for the argument-details.\n')
		ica_data = pd.DataFrame(ica_transf_X, columns= ['Component_'+ str(x) for x in range(1,n_features+1)])
		ica_data['D_Variable'] = array[:,(len(array[1])-1)]
		return ica_data

#	### Neighborhood Component Analysis
#	elif (transform == 'NCA'):
#		nca = NeighborhoodComponentsAnalysis(n_components=n_features, random_state=42)
#		nca = nca.fit(X, Y)
#		nca_transf_X = nca.transform(X)
#		nca_det= '''
#******** NCA Details *********\n
#* NCA Number of Iterations:\n
#{0}\n		
#* NCA Components:\n
#{1}\n					
#'''.format(nca.n_iter_, nca.components_)
#		if (details == 'yes'):
#			print(nca_det)
#		elif (details == 'no'):
#			print('No request for NCA Details.\n')
#		else:
#			print('=> Error: Invalid input for the argument-details.\n')
#		nca_data = pd.DataFrame(nca_transf_X, columns= ['Component_'+ str(x) for x in range(1,n_features+1)])
#		nca_data['D_Variable'] = array[:,(len(array[1])-1)]
#		return nca_data
#
#	### When the input for the argument transform is invalid
#	else:
#		print('=> Error: Invalid input for the argument-details or transform.\n\n')
#

from IPython.display import display, Javascript
from google.colab.output import eval_js
from base64 import b64decode

def take_photo(filename='photo.jpg', quality=0.8):
  js = Javascript('''
    async function takePhoto(quality) {
      const div = document.createElement('div');
      const capture = document.createElement('button');
      capture.textContent = 'Capture';
      div.appendChild(capture);

      const video = document.createElement('video');
      video.style.display = 'block';
      const stream = await navigator.mediaDevices.getUserMedia({video: true});

      document.body.appendChild(div);
      div.appendChild(video);
      video.srcObject = stream;
      await video.play();

      // Resize the output to fit the video element.
      google.colab.output.setIframeHeight(document.documentElement.scrollHeight, true);

      // Wait for Capture to be clicked.
      await new Promise((resolve) => capture.onclick = resolve);

      const canvas = document.createElement('canvas');
      canvas.width = video.videoWidth;
      canvas.height = video.videoHeight;
      canvas.getContext('2d').drawImage(video, 0, 0);
      stream.getVideoTracks()[0].stop();
      div.remove();
      return canvas.toDataURL('image/jpeg', quality);
    }
    ''')
  display(js)
  data = eval_js('takePhoto({})'.format(quality))
  binary = b64decode(data.split(',')[1])
  with open(filename, 'wb') as f:
    f.write(binary)
  return filename

#@title Example form fields
#@markdown Forms support many types of fields.

no_type_checking = ''  #@param
string_type = 'example'  #@param {type: "string"}
slider_value = 142  #@param {type: "slider", min: 100, max: 200}
number = 102  #@param {type: "number"}
date = '2010-11-05'  #@param {type: "date"}
pick_me = "monday"  #@param ['monday', 'tuesday', 'wednesday', 'thursday']
select_or_input = "apples" #@param ["apples", "bananas", "oranges"] {allow-input: true}
#@markdown ---

from IPython.display import Image
try:
  filename = take_photo()
  print('Saved to {}'.format(filename))
  
  # Show the image which was just taken.
  display(Image(filename))
except Exception as err:
  # Errors will be thrown if the user does not have a webcam or if they do not
  # grant the page permission to access it.
  print(str(err))

# Importing the details about the Wisconsin Breast Cancer
wbc_text_url = "https://raw.githubusercontent.com/eadadzie/Tech_Talent_South-AC_Data_Science/master/PROJECT/2_ML_Project/Datasets/Breat_cancer_Wisconsin/w_prog_bc.txt"
page_wbc = requests.get(wbc_text_url)
print(page_wbc.text)

# Importing the Wisconsin Breast Cancer datasets
headers_bc = []
df_wbc = pd.read_csv("https://raw.githubusercontent.com/eadadzie/Tech_Talent_South-AC_Data_Science/master/PROJECT/2_ML_Project/Datasets/Breat_cancer_Wisconsin/w_prog_bc.csv", header=None)
# print(df_wbc)
# print()

# Importing the details about the Wine Datasets
wine_text_url = "https://raw.githubusercontent.com/eadadzie/Tech_Talent_South-AC_Data_Science/master/PROJECT/2_ML_Project/Datasets/Wine_Quality/winequality.txt"
# page_wine = requests.get(wine_text_url)
# print(page_wine.text)

# Importing the Wine data
df_wine = pd.read_csv("https://raw.githubusercontent.com/eadadzie/Tech_Talent_South-AC_Data_Science/master/PROJECT/2_ML_Project/Datasets/Wine_Quality/winequality-red.csv")
# print(df_wine)
# print()

##### Importing the details about the Students' Performance
student_text_url = "https://raw.githubusercontent.com/eadadzie/Tech_Talent_South-AC_Data_Science/master/PROJECT/2_ML_Project/Datasets/Students_performance/student.txt"
page_student = requests.get(student_text_url)
print(page_student.text)

# Importing the Student data

#print('*** Datasets on Math\n')
df_student_mat = pd.read_csv("https://raw.githubusercontent.com/eadadzie/Tech_Talent_South-AC_Data_Science/master/PROJECT/2_ML_Project/Datasets/Students_performance/student-mat.csv", delimiter=";")
df_student_mat.insert(1, "course", "Math")
#print(df_student_mat)
print()

#print('*** Datasets on Portugese\n')
df_student_port = pd.read_csv("https://raw.githubusercontent.com/eadadzie/Tech_Talent_South-AC_Data_Science/master/PROJECT/2_ML_Project/Datasets/Students_performance/student-por.csv", delimiter=";")
df_student_port.insert(1, "course", "Portuguese")
#print(df_student_port)
print()

# Combined data from both subject courses
print('*** Full Student Datasets\n')
df_student = df_student_mat.append(df_student_port)
print(df_student)
print()

##### EXPLORATORY DATA ANALYSIS - DETAILS OF DATASETS

### Details of the data
print("*** Details of Dataset\n")
print(df_student.info())
print("\n")

### Capture all categorical varibles and get their unique values
print("*** Unique values in categorical variables\n")

col_unique_vals = {}
for col in df_student.columns:
  if df_student[col].dtypes == object:
    col_unique_vals[col] = df_student[col].unique()
    if len(col) <= 4:
      print(col, "\t:\t", df_student[col].unique())
    else:
      print(col, ":\t", df_student[col].unique())

#print("Unique Values:\n", col_unique_vals)
print("\n")

### Checking Missing Values
print("*** Missing Values\n")
print(df_student.isnull().sum())
print("\n")

### Descriptive Stats of Continues variables
print("*** Descriptive Stats\n")
print(df_student.describe())
print("\n")

##### EXPLORATORY DATA ANALYSIS - VISUALIZATION

num_variables = [name for name in df_student.columns if df_student[name].dtypes == int]
#print(num_variables)

### Plotting Histogram
print("*** Histogram Plot")
count = 0
n_rows = 4
n_cols = 4
fig, axes=plt.subplots(n_rows, n_cols, figsize=(16,14), sharex=False, gridspec_kw={'hspace': 0.35, 'wspace': 0.35})
for i in range(len(num_variables)):
  sns.histplot(df_student[num_variables[i]], ax=axes[i//n_cols, i%n_cols])
  count += 1
plt.show()
print("\n")

### Plotting Factor Plots
print("*** Factor Plots")
## Considering Five Factors
third_factor = ["school", "address", "studytime", "failures", "internet"]
# Looping through the factors
for factor in third_factor:
  with sns.axes_style(style='ticks'):
      g = sns.factorplot(x="course", y="G3", hue="sex", col= factor, data=df_student, kind="box")
      g.set_axis_labels("Course", "Final Grade");
  plt.show()
  print()
print("\n")

### Plotting Pair Plots
print("*** Pair Plots")
sns.pairplot(df_student[["age", "absences", "G1", "G2", "G3","sex"]], hue='sex', size=2.5);
plt.show()
print("\n")

### Plotting Correlation Matrix
print("*** Correlation Matrix")

fig1, ax1 = plt.subplots(figsize=(16,14))
#matrix = np.triu(df.corr())
ax1 = sns.heatmap(df_student.corr(), vmin=-1, vmax=1, annot=True, cmap='BrBG', mask=None, annot_kws={"size":8})
ax1.set_title('Correlation Heatmap', fontdict={'fontsize':12}, pad=12)
ax1.set_xticklabels(ax1.get_xmajorticklabels(), fontsize = 8, rotation=90)
ax1.set_yticklabels(ax1.get_ymajorticklabels(), fontsize = 8, rotation=0)
plt.show()

##### EXPLORATORY DATA ANALYSIS - PREPROCESSING AND FEATURE ENGINEERING

### Clean Data
df_student_clean = clean_data(df_student, mv_list=[99999], output='removeNaN', equation='IQRE', selected_cols=["age", "absences", "G1", "G2", "G3"], details= 'yes')
print("*** Clean Data\n", df_student_clean)
print("\n")

### Transform Data

### Extract or Select Features

### Plotting Histogram
print("*** Histogram Plot")
count = 0
n_rows = 4
n_cols = 4
fig, axes=plt.subplots(n_rows, n_cols, figsize=(16,14), sharex=False, gridspec_kw={'hspace': 0.35, 'wspace': 0.35})
for i in range(len(num_variables)):
  sns.histplot(df_student_clean[num_variables[i]], ax=axes[i//n_cols, i%n_cols])
  count += 1
plt.show()
print("\n")

### Plotting Factor Plots
print("*** Factor Plots")
## Considering Five Factors
third_factor = ["school", "address", "studytime", "failures", "internet"]
# Looping through the factors
for factor in third_factor:
  with sns.axes_style(style='ticks'):
      g = sns.factorplot(x="course", y="G3", hue="sex", col= factor, data=df_student_clean, kind="box")
      g.set_axis_labels("Course", "Final Grade");
  plt.show()
  print()
print("\n")

### Plotting Pair Plots
print("*** Pair Plots")
sns.pairplot(df_student_clean[["age", "absences", "G1", "G2", "G3","sex"]], hue='sex', size=2.5);
plt.show()
print("\n")

### Plotting Correlation Matrix
print("*** Correlation Matrix")

fig1, ax1 = plt.subplots(figsize=(16,14))
#matrix = np.triu(df.corr())
ax1 = sns.heatmap(df_student_clean.corr(), vmin=-1, vmax=1, annot=True, cmap='BrBG', mask=None, annot_kws={"size":8})
ax1.set_title('Correlation Heatmap', fontdict={'fontsize':12}, pad=12)
ax1.set_xticklabels(ax1.get_xmajorticklabels(), fontsize = 8, rotation=90)
ax1.set_yticklabels(ax1.get_ymajorticklabels(), fontsize = 8, rotation=0)
plt.show()
print("\n")

### Box Plot
print("*** Box Plot of Outputs")
sns.boxplot(data=df_student_clean[["G1", "G2", "G3"]], orient="v", palette="Set2")
plt.show()
print("\n")

##### INFERENTIAL STATS
from statsmodels.graphics.factorplots import interaction_plot
import matplotlib.pyplot as plt

import statsmodels.api as sm
from statsmodels.formula.api import ols

### Perform 2k Factorial Design on the following factors (Repeated Measures ANOVA) 
# Course
# Sex
# Address
# School

df_sample = df_student_clean[["sex", "address", "school", "course", "G3"]]
print("Sample Data:\n", df_sample)
print("\n")

print("*** Group Means\n")
print("* Sex:\n", df_sample.groupby(["sex"]).mean())
print()
print("* School:\n", df_sample.groupby(["school"]).mean())
print("\n")
print("* Course:\n", df_sample.groupby(["course"]).mean())
print("\n")
print("* Address:\n", df_sample.groupby(["address"]).mean())
print("\n")

##### Perform Four-way ANOVA (Non-Repeated Measure)
print("*** Between Subject ANOVA")
main_factors  = 'G3 ~ C(sex) + C(address) + C(school) + C(course) + ' 
two_way_int   = 'C(sex):C(address) + C(sex):C(school) + C(sex):C(course) + C(address):C(school) + C(address):C(course) + C(school):C(course) + '
three_way_int = 'C(sex):C(address):C(school) + C(sex):C(address):C(course) + C(sex):C(school):C(course) + C(address):C(school):C(course) + '
four_way_int  = 'C(sex):C(address):C(school):C(course)'

formula = main_factors + two_way_int + three_way_int + four_way_int

model = ols(formula, data=df_sample).fit()
anova_model = sm.stats.anova_lm(model, typ=2)
print(anova_model)



# ##### Perform Four-way ANOVA (Repeated Measure)
# from statsmodels.stats.anova import AnovaRM
# model_rm = AnovaRM(data=df_sample, depvar='G3', subject='None', within=['course']).fit()
# print(model_rm)

signif = []
for i in anova_model.iloc[:, 3]:
  if(i < 0.05):
    signif.append("True")
  else:
    signif.append("False")
anova_model["signficant"]=signif

anova_model

### Ploting the Residuals
print("*** Plotting Residuals\n")
res = model.resid
sm.qqplot(res, line='s')
plt.xlabel("Theoretical Quantiles")
plt.ylabel("Standardized Residuals")
plt.show()
print("\n")

### Plotting the Interactions
# Two-way interaction
print("*** Two-way Interaction\n")
g = sns.catplot(x="course", y="G3", hue="sex",
                kind= "point", data=df_sample)
plt.title("Two-way Significant Interaction")
plt.ylabel("Final Grade")
plt.show()
print("\n")

# Three-way Interaction
print("*** Three-way Significant Interaction\n")
g = sns.catplot(x="course", y="G3", hue="sex",
                col="school", kind= "point", data=df_sample)
plt.show()

### Perform Assumption Checks before the Factorial Design

# We can assume independence in our observations. The performance of one student should not determine the performance of another student.
# However, there may be latent variables (such as student participation) which could influence student performance.

# Normality of Dependent Variable
sns.histplot(df_sample[["G3"]])
plt.xlabel("Final Grade")
plt.title("Distribution of G3")
plt.show()
print(stats.normaltest(df_sample["G3"]))
print("G3 is not normally distributed.")

# Heteroskedasticity (Variance Checks)
sns.histplot(res)
plt.xlabel("Residuals")
plt.title("Distribution of Residuals")
plt.show()


import scipy.stats as stats
print(stats.normaltest(df_sample["G3"]))
print("The residuals are not normally distributed.")

print("*** Between Subject ANOVA")
main_factors  = 'G3 ~ C(sex) + C(address) + C(school) + C(course) + ' 
two_way_int   = 'C(sex):C(address) + C(sex):C(school) + C(sex):C(course) + C(address):C(school) + C(address):C(course) + C(school):C(course) + '
three_way_int = 'C(sex):C(address):C(school) + C(sex):C(address):C(course) + C(sex):C(school):C(course) + C(address):C(school):C(course) + '
four_way_int  = 'C(sex):C(address):C(school):C(course)'
formula = main_factors + two_way_int + three_way_int + four_way_int

model = ols(formula, data=df_sample).fit()
anova_model = sm.stats.anova_lm(model, typ=2, robust="hc0")
anova_model

signif = []
for i in anova_model.iloc[:, 3]:
  if(i < 0.05):
    signif.append("True")
  else:
    signif.append("False")
anova_model["signficant"]=signif
anova_model

##### PREDICTIVE MODELING WITH MACHINE LEARNING ALGORITHMS
from sklearn import preprocessing
from sklearn.model_selection import KFold
from sklearn.model_selection import cross_val_score
from sklearn.pipeline import Pipeline
from sklearn.pipeline import FeatureUnion
from sklearn.decomposition import PCA
from sklearn.feature_selection import SelectKBest
from sklearn.metrics import r2_score
from matplotlib import pyplot

from sklearn.linear_model import LinearRegression, LogisticRegression
from sklearn.tree import DecisionTreeRegressor
from sklearn.neighbors import KNeighborsRegressor
from sklearn.svm import SVR
from sklearn.linear_model import BayesianRidge
from sklearn.ensemble import GradientBoostingRegressor, AdaBoostRegressor, RandomForestRegressor, ExtraTreesRegressor

# Create an ML model with two outputs (both discrete and continuous) that are directly related.
# Create a pipeline that will integrate the ML algorithms and the Feature-engineered input and output
# Tune the models' hyper-parameter
# Optimize based on the features selected and the algorithm parameters
# Perform Y-scrambling to validate the models' performance against pure-chance

df_student_cat = df_student_clean.select_dtypes(include=[object])
df_student_num = df_student_clean.select_dtypes(include=[int])
print("Categorical Variables:\n", df_student_cat)
# print()
# print("Numeric Variables:\n", df_student_num)

# creating column names for OneHot Encoder
cat_col_names = []
for k, v in col_unique_vals.items():
  cat_col_names.extend([f"{k}_{val}" for val in v])

print("Categorical Columns:", len(cat_col_names))
print("\n")

### Label Encoder
le = preprocessing.LabelEncoder()
df_student_LE = df_student_cat.apply(le.fit_transform)
# print("Label Encoder:\n", df_student_LE)
# print()

### One Hot Encoder
enc = preprocessing.OneHotEncoder()
enc.fit(df_student_cat)
onehotlabels = enc.transform(df_student_cat).toarray()

# Create Dataframe
df_onhot_cat = pd.DataFrame(onehotlabels, columns=cat_col_names)
# print("OneHot Dataset:\n", df_onhot_cat)
# print("\n")

# Integrate Datasets
df_student_enc = pd.merge(df_onhot_cat, df_student_num, how="inner", on=[df_onhot_cat.index])
df_student_enc.drop(columns=["key_0"], inplace=True)
print("Final Encoded Dataset:\n", df_student_enc)
print("\n")

# PCA-Transformed Data
df_pca = feature_engineering(df_student_enc, n_features= 20, transform='PCA', score_fn=f_regression, details='no')
print("PCA Data:\n", df_pca)
print("\n")

# ICA-Transformed Data
df_ica = feature_engineering(df_student_enc, n_features= 9, transform='ICA', score_fn=f_regression, details='no')
print("ICA Data:\n", df_ica)
print("\n")

# UFS Data
df_ufs = feature_engineering(df_student_enc, n_features= 6, transform='UFS', score_fn=f_regression, details='no')
print("UFS Data:\n", df_ufs)
print("\n")


# Function for ML Model
def ml_model(dataframe, label="Data"):
  ### Creating Pipeline Models
  array = dataframe.values
  X = array[:, :len(dataframe.columns) -1]
  Y = array[:, len(dataframe.columns) -1]

  # ML Models
  models = []
  models.append(('KNN', KNeighborsRegressor()))
  #models.append(('CART', DecisionTreeRegressor()))
  models.append(('NB', BayesianRidge()))
  models.append(('SVM', SVR(C=1.0, epsilon=0.2)))
  models.append(('GB', GradientBoostingRegressor(learning_rate=0.05, n_estimators=51)))
  models.append(('AB', AdaBoostRegressor(n_estimators=51, random_state=0)))
  models.append(('RF', RandomForestRegressor()))
  models.append(('LR', LinearRegression()))

  # evaluate each model in turn
  results = []
  names = []
  scoring = 'r2'

  df_res = pd.DataFrame(columns=["Algorithm", "Mean", "STD"])
  for name, model in models:
    kfold = KFold(n_splits=10, random_state=7)
    cv_results = cross_val_score(model, X, Y, cv=kfold, scoring=scoring)
    results.append(cv_results)
    names.append(name)
    #msg = "%s: %f (%f)" % (name, cv_results.mean(), cv_results.std())
    #print(msg)
    df_res = df_res.append({"Algorithm":name, "Mean":cv_results.mean(), "STD":cv_results.std()}, ignore_index=True)
  print("Results:\n", df_res)
  print("\n")

  # boxplot algorithm comparison
  fig = pyplot.figure()
  fig.suptitle(f'Algorithm Comparison - {label}')
  ax = fig.add_subplot(111)
  pyplot.boxplot(results, showmeans=True)
  ax.set_xticklabels(names)
  pyplot.show()

### Using Raw data
print("*** Raw Data Model")
ml_model(df_student_enc, label="Raw Data")
print("\n")

print("*** UFS Data Model")
ml_model(df_ufs, label="UFS")
print("\n")

print("*** PCA Data Model")
ml_model(df_pca, label="PCA")
print("\n")

print("*** ICA Data Model")
ml_model(df_ica, label="ICA")
print("\n")








# create feature union
# features = []
# features.append(('select_best', SelectKBest(k=3)))
# features.append(('pca', PCA(n_components=6)))
# feature_union = FeatureUnion(features)
# df_feat_union = feature_union.fit_transform(X, Y)
#print("Unioned Features", df_feat_union)

# create pipeline
# estimators = []
# estimators.append(('feature_union', feature_union))
# estimators.append(('KNN', KNeighborsRegressor()))
# estimators.append(('CART', DecisionTreeRegressor()))
# estimators.append(('NB', BayesianRidge()))
# estimators.append(('SVM', SVR(C=1.0, epsilon=0.2)))
# model = Pipeline(estimators)

# evaluate pipeline
# kfold = KFold(n_splits=10, random_state=7)
# results = cross_val_score(model, X, Y, cv=kfold, scoring="r2")
# print("*** Results", results.mean())

